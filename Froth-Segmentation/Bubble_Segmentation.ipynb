{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03252c10-9b47-4140-b615-cd898ed9caaf",
   "metadata": {},
   "source": [
    "## Setup Steps\n",
    "\n",
    "- Open Jupyter Lab via. https://hemera5.fz-rossendorf.de/ or open your private Jupyter Lab\n",
    "- Open a Terminal in Jupyter Lab for commands\n",
    "- Make a symlink: ln -s /bigdata/flotsim   # Later in Jupyter Lab: /home/beyer83/bigdata/flotsim available <br>\n",
    "    \n",
    "#### Setup Anaconda environment for Jupyter Lab\n",
    "- see: https://fwcc.pages.hzdr.de/infohub/hpc/interactive.html#kernelpic\n",
    "- module load anaconda\n",
    "- conda init bash\n",
    "- exec bash\n",
    "- conda create -n 'BubbleSeg' python=3.10.4\n",
    "- conda activate BubbleSeg\n",
    "- pip install ipython ipywidgets==7.7.0 pillow numpy matplotlib tqdm opencv-contrib-python scikit-image gputils albumentations\n",
    "- conda install ipykernel ipython_genutils\n",
    "\n",
    "##### Install Tensorflow and Stardist\n",
    "- https://www.tensorflow.org/install/pip\n",
    "- conda install -c conda-forge cudatoolkit=11.8.0\n",
    "- pip install nvidia-cudnn-cu11==8.6.0.163\n",
    "- pip install tensorflow==2.13.*\n",
    "- pip install stardist\n",
    "\n",
    "##### Check for GPU (On Jupyter Session with GPU):\n",
    "- nvidia-smi\n",
    "- print(tf.config.list_physical_devices('GPU')) --> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "##### Build Kernel Jupyter Interface\n",
    "- python -m ipykernel install --user --name BubbleSeg --display-name 'BubbleSeg'\n",
    "- Activate Anaconda env in Jupyter Lab: Kernel --> Change Kernel --> BubbleSeg\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bdbcd-ec19-465d-97a3-fd0ce59ef4a4",
   "metadata": {},
   "source": [
    "## Sources: \n",
    "   - Utility: ChatGPT\n",
    "   - IPywidets: https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html#Tabs\n",
    "   - skimage: https://scikit-image.org/docs/stable/api/api.html\n",
    "   - Stardist: https://github.com/stardist/stardist\n",
    "   - Stardist: https://www.youtube.com/watch?v=Amn_eHRGX5M\n",
    "   - Stardist Examples: https://github.com/stardist/stardist/blob/master/examples\n",
    "   \n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3dc282-3bcd-4480-a306-d62c2a3a5a99",
   "metadata": {},
   "source": [
    "## Guide to Data\n",
    "  - try to look into the data before reading the specific details of this guide.\n",
    "  - Skim the PDF and Excel documentation and try the notebook for exploratory data analysis.\n",
    "    \n",
    "### Data Description\n",
    "  - For every of 62 experiments up to 50000 .npy grayscale images were saved.\n",
    "  - All images have the shape (2056, 2464,1) and occupy ~4.8 MB of disk space.\n",
    "  - Camera 'ABE' records from the top; camera '253' records from the side through a plexiglass tube.\n",
    "  - Each experiment changes bubble parameters (look in Excel file); they still look nearly the same.\n",
    "  - In each experiment 4  different types of concentrates are tested; they represent how crushed minerals are extracted.\n",
    "  - There are missing pictures, the camera timestamps are broken, the cameras do NOT record in sync.\n",
    "  - A froth scraper occasionally scrapes of old froth, darkening the image and resetting the bubble buildup.\n",
    "   \n",
    "### Findings from Data Analysis\n",
    "  - Data is extremly diverse!\n",
    "  - Bubbles can be foamy, dark, bright, squished, transparent, bursting, round, polygonal and blurred\n",
    "  - Images have a lot of hidden noise --> Edge detection filters fail\n",
    "  - In edge cases bubbles can be slightly larger than 512x512 pixels. (just 3 instances were found in experiment 20)\n",
    "  - Bubble sizes range from 5 to 600 pixels.\n",
    "  - There can be 5 - 350 bubbles instances per 512x512 image.\n",
    "  - Every bubble has a reflection point and varying outline reflection\n",
    "\n",
    "#### What did not work:\n",
    "  - Edge detection (too noisy)\n",
    "  - Sharpening (removes details, noise only a small problem)\n",
    "  - Noise reduction with Non-Local Means (removes details from bubbles irregularily, removes small bubbles)\n",
    "        \n",
    "#### Concentrate 1:\n",
    "   - Nearly impossible to label. Has very distorted bubbles completly covered in dust.\n",
    "        \n",
    "#### Concentrate 2:\n",
    "   - Has a lot of dust froth several mm thick.\n",
    "   - Has bubble sizes from small to large\n",
    "   - This froth covers most of the bubbles. Often only the top of bubbles is visible.\n",
    "\n",
    "#### Concentrate 3:\n",
    "   - Concentrate with the most bubble types\n",
    "   - Bubble types range from slightly foamy to dusty to nearly clear.\n",
    "   - Has bubble sizes from fine to large.\n",
    "   \n",
    "#### Concentrate 4:\n",
    "   - Concentrate with super transparent bubbles, 2nd layer bubbles are clearly visible but not of interest!\n",
    "   - All bubbles are clear with sizes from fine to large.\n",
    "   \n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48940453-3de0-4197-8579-5abe5dccde35",
   "metadata": {},
   "source": [
    "## Guide to Data Preparation\n",
    "\n",
    "### Data Format for Stardist\n",
    "   - Checkout: https://github.com/stardist/stardist and https://www.youtube.com/watch?v=Amn_eHRGX5M\n",
    "   - Only .tif images!\n",
    "   - Stardist needs square train images of side lengths like [124,248,512,1024,...].\n",
    "   - Usually 100 instances per class are enough for Stardist with its U-Net model structure.\n",
    "   - Instances need to be star-convex\n",
    "\n",
    "### Model Inference:\n",
    "   - It is planned to have one separate model per camera and concentrate as this improves accuracy.\n",
    "   - Concentrate 1 will not be used as its data is nearly unusable.\n",
    "   - This leaves the concentrate models [ABE_c2, ABE_c3, ABE_c4, 253_c2, 253_c3, 253_c4]\n",
    "   - Optionally entire camera models can be trained for easier usage: [ABE_c2,c3,c4, 253_c2,c3,c4, ABE,253_c2,c3,c4]\n",
    "\n",
    "### How to Handpick the Right Data\n",
    "   - Use the handy UI below to download image subsamples!\n",
    "   - Handpick separately for each camera ID and concentrate\n",
    "   - For experiment 20 concentrate numbers are noted as follows: {c1 : 0-2920, c2 : 2920-4720, c3 : 4720-11860, c4 : 11860-end}\n",
    "   - If handpicking from other experiments, please fill in the threshold inside the functinon download_handpick_img()!\n",
    "   - Make sure to extract a lot of pictures with big bubbles as they there are maybe just 2-5 instance per image!\n",
    "   - Make sure to include examples for all the bubble types, if very accurate predictions are wished:\n",
    "   - Thus Include tiny, small, big, foamy, dark, bright, squished, transparent, bursting, round, polygonal and blurred bubbles!\n",
    "   - May pick subsamples of start and end of concentrate phases (early phases have more dust which cahnges with time)!\n",
    "   - Include subsamples from multiple parts of the image (top, botton, left, rigth, ...): different blur, lighting and squishing.\n",
    "   - There is may no big need to pick data from more than one experiment setting.\n",
    "   \n",
    "#### Handpicked Data\n",
    "\n",
    "   - about 40-60 images were picked for each of the six model-concentrate combinations\n",
    "   - Its goal is to include enough diverse bubble structures that can easily be picked for labeling\n",
    "\n",
    "### How Label the Data\n",
    "   - Label images with Fiji, Labkit, or QuPath (QuPath seems the best)\n",
    "   - 15 to 25 images are enough per camera-concentrate\n",
    "   - Only label the 1st layer bubbles, ignore bubbles below\n",
    "   - Label all parts of 1st layer bubbles: Label parts of 1st layer bubbles covered by froth or other transparent bubbles\n",
    "   - Do not try to label data that is impossible to label\n",
    "   - Plan to spend 1.5 or even 3.0 hours to label a single picture!\n",
    "   - The smallest bubbles you have to label are about 4x4 pixels and generally all that can be recognized by the human eye!\n",
    "   \n",
    "### Adjust Current Labels\n",
    "   - Open the QuPath project file with QuPath to see and adjust the labels\n",
    "   - The image loading paths may be changed. On opening the project a window opens highlighting the missing paths.\n",
    "   - QuPath is pretty good at recognizing the changed image file paths. If it recognized the paths, you can easily use the proposed replacement URIs.\n",
    "   - If no replacement URIs show up, drag the handpicked images with the correct name into this window and the correct replacement URIs show up.\n",
    "   - Now, drag every handpicked images to this window\n",
    "   \n",
    "   \n",
    "  \n",
    "#### Exporting Binary masks:\n",
    "   - If using QPath use this script: https://github.com/stardist/stardist/blob/master/extras/qupath_export_annotations.groovy\n",
    "   - Else refer to: https://github.com/stardist/stardist#annotating-images\n",
    "   \n",
    "#### Labeled Data\n",
    "\n",
    "   - Labels are available in QPath project files and can easily be corrected.\n",
    "   - Please consider to check all images for missing labels before the downstream applciation, especially for c2 where a lot of 4x4 to 10x10 bubbles can easily be missed!\n",
    "   - Also consider annotating a few more images for testing and for validation! Currently all 10 images are used solely for training!\n",
    "   - One model should have around 25 images so that tiny, small, medium and especially big bubbles can be accurately predicted.\n",
    "   - Only 10 images just for concentrates c2 and c4 are labeled (takes very long: 1.5 to 3 hours per image!)\n",
    "   - Images for validation and testing could be picked too, later.\n",
    "   - Currently labeled images are {ABE_c1 : [], ABE_c2 : [1,2,3,4,8,11,18], ABE_c3 : [], ABE_c4 : [31,38,41], 253_c1 : [], 253_c2 : [], 253_c3 : [], 253_c4 : []}\n",
    "   \n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd05a2-3678-40ad-96cb-52e7145e5bd0",
   "metadata": {},
   "source": [
    "## Results:\n",
    "   - The first model prototype already yields good predictions for cam ABE.\n",
    "   - It can classify common bubble types pretty accurate and is even able to classify images of camera 253 slightly well.\n",
    "   - Predictions are especially inaccurate for big bubbles, transparent bubbles, frothy bubbles, dark bubbles, tiny bubbles as these were not abundantly present in the training data at all.\n",
    "   - Make sure to label more images with these bubble types!\n",
    "   - The model sees bubbles on dark or gray edges of the experiment setup.\n",
    "   - Add these areas definetly to your training data and maybe handpick a nealy black area!\n",
    "    \n",
    "### Further improvements:\n",
    "   - Actually train the model until it actually converges (model could only be trained for 250 epochs before 9h time out)\n",
    "   - Manage to setup a GPU support for Stardist (failed because of CuDNN version mismatch for us)\n",
    "   - If wished to analyze the bubbles even while froth-scraping, it will need an additional +7 images for models to learn the scraper interference!\n",
    "   - Else exclude every froth-scraping image from analysis!\n",
    "   - May correct some of the labelings for the few small bubbles that are missing a label. \n",
    "   - Else, the model will predict small bubbles less often!\n",
    "   - Validation data was not present to determine the best model epochs! Include +5 val pictures.\n",
    "   - When validation data is present, uncomment the line \"model.optimize_thresholds(val_x, val_y)\" to improve model performance.\n",
    "   - The model cannot predict the occasional enormous bubbles of over 512x512 pixels. If the labeling labour is really worth it, train Stardist with 1024x1024 images.\n",
    "   - See these enourmous bubbles, for example, at [E20_ABE_img5329, E20_253_img19580, E15_ABE_img4912, ...]\n",
    "   \n",
    "### Further Work:\n",
    "   - Finish Starist work as described above.\n",
    "   - May use Multistar for overlapping instance segmentation.\n",
    "   \n",
    "#### Considerations for Multistar\n",
    "   - The labels need to be reworked to consider bubbles under the first layer of bubbles (2nd layer bubbles are new instances)\n",
    "   - It might work more well to analyze the bubble images, if 1st and 2nd and 3rd layer can be distinguished\n",
    "   - There is only the Multistar paper and a less well documented simple GitHub reposotory.\n",
    "   \n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c2a0d4-63eb-4e6f-94e3-f6536f347861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 10:12:41.605134: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-15 10:12:42.889173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets \n",
    "from IPython.display import display, clear_output\n",
    "from IPython.display import Image as IPython_Image\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents, gputools_available, relabel_image_stardist\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "import albumentations as A\n",
    "from PIL import Image as PIL_Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from tifffile import imread\n",
    "import io\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage import filters, feature\n",
    "from csbdeep.utils import normalize\n",
    "from csbdeep.data import Normalizer, normalize_mi_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811438f-c5ef-4995-baeb-78ccc60efdba",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db9fd52-9ed1-448e-909e-ab4b7ecf7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "### Utility Functions ###\n",
    "#########################\n",
    "\n",
    "def load_pic_data(messreihe, base_dir_path, cam_id, pic_num):\n",
    "    dir_names = [name for name in os.listdir(base_dir_path) if f'_{messreihe}_' in name]\n",
    "    if len(dir_names) == 0:\n",
    "        print(f'No dirs found for {base_dir_path} and messreihe: {messreihe}.')\n",
    "        return np.nan\n",
    "    else:\n",
    "        dir_name = dir_names[0]\n",
    "        dir_path = f'{base_dir_path}/{dir_name}'\n",
    "        fn = [name for name in os.listdir(dir_path) \n",
    "                        if (f'_{pic_num}_' in name) \n",
    "                       and (f'{cam_id})'   in name)]\n",
    "        if len(fn) == 0:\n",
    "            return np.nan # No picture exists for pic_num (out of range or measurement gap)\n",
    "        else:\n",
    "            fn = fn[-1] # -1 for latest experiment retake\n",
    "            file_path = f'{dir_path}/{fn}'\n",
    "            data = np.load(file_path)\n",
    "            return data\n",
    "\n",
    "def make_tiff(messreihe, base_dir_path, cam_id, pic_range): \n",
    "    pil_images = []\n",
    "    if len(pic_range) > 800+1: # 800 --> ~4 GB\n",
    "        raise Exception('Warning: TIFF files can only contain up to 800 grayscale imagese before reaching the file size limit!')\n",
    "    for i in tqdm(pic_range, desc=\"Reading Images\"): #\n",
    "        img = load_pic_data(messreihe, base_dir_path, cam_id, pic_num=i) # This takes the most time! --> 95%\n",
    "        img = np.squeeze(img, axis=-1) # shape (2056, 2464, 1) --> (2056, 2464)\n",
    "        pil_images.append(PIL_Image.fromarray(img, mode='L')) # 'L' for grayscale images\n",
    "        \n",
    "    tiff  = PIL_Image.new('L', (pil_images[0].width, pil_images[0].height * len(pil_images)))\n",
    "    for i, pil_image in enumerate(tqdm(pil_images, desc=\"Converting Images\")):\n",
    "        tiff.paste(pil_image, (0, i * pil_image.height))\n",
    "    for i in tqdm(range(1), desc='Saving Images'):\n",
    "        tiff.save('combined_image.tiff')\n",
    "        \n",
    "def make_png(messreihe, base_dir_path, cam_id, pic_range): \n",
    "    pil_images = []\n",
    "    if len(pic_range) > 800+1: # 800 --> ~4 GB\n",
    "        raise Exception('Warning: TIFF files can only contain up to 800 grayscale imagese before reaching the file size limit!')\n",
    "    for i in tqdm(pic_range, desc=\"Reading Images\"): #\n",
    "        img = load_pic_data(messreihe, base_dir_path, cam_id, pic_num=i) # This takes the most time! --> 95%\n",
    "        img = np.squeeze(img, axis=-1) # shape (2056, 2464, 1) --> (2056, 2464)\n",
    "        pil_images.append(PIL_Image.fromarray(img, mode='L')) # 'L' for grayscale images\n",
    "        \n",
    "    tiff  = PIL_Image.new('L', (pil_images[0].width, pil_images[0].height * len(pil_images)))\n",
    "    for i, pil_image in enumerate(tqdm(pil_images, desc=\"Converting Images\")):\n",
    "        tiff.paste(pil_image, (0, i * pil_image.height))\n",
    "    for i in tqdm(range(1), desc='Saving Images'):\n",
    "        tiff.save('combined_image.tiff')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83270cbe-eb85-4b58-a6d6-ba2521341e8a",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427127a-5579-4e2c-a87d-953668525343",
   "metadata": {},
   "source": [
    "## Interactive Dashboard for Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ab6a3-7245-4bff-bd8c-99e9f48da3a4",
   "metadata": {},
   "source": [
    "#### Tab 1: Data Preview and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2b10f-d390-4f00-8e17-2275233a155c",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa1d129-bb5a-437d-ada6-76e77a676358",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "########### Defined Ipywidget Components ###########\n",
    "####################################################\n",
    "\n",
    "dropdown        = widgets.Dropdown(options=['ABE', '253'], description='Camera ID:')\n",
    "input_path      = widgets.Text(value='/home/beyer83/flotsim/Camera_Outotec', description='base_dir_path:')\n",
    "input_messreihe = widgets.IntSlider(value=20, min=1, max=62, description='Experiment Num.:')\n",
    "input_pic_num   = widgets.IntSlider(value=1, description='Picture num:')\n",
    "input_disp_size = widgets.FloatSlider(value=0.25, min=0.01, max=1, step=0.01, description='Display Size (%):')\n",
    "\n",
    "button_save                = widgets.Button(description='Downl. Image')\n",
    "button_recommended_fparams = widgets.Button(description='Use Rec. Filters')\n",
    "\n",
    "########################\n",
    "### Filter Accordion ###\n",
    "########################\n",
    "\n",
    "### Crop Filter\n",
    "title_253 = widgets.HTML(\"<h2>CamID 253:</h2>\")\n",
    "range_slider_accordA_x_253 = widgets.IntRangeSlider(description=f'X Crop')\n",
    "range_slider_accordA_y_253 = widgets.IntRangeSlider(description=f'Y Crop')\n",
    "\n",
    "title_ABE = widgets.HTML(\"<h2>CamID ABE:</h2>\")\n",
    "range_slider_accordA_x_ABE = widgets.IntRangeSlider(description=f'X Crop')\n",
    "range_slider_accordA_y_ABE = widgets.IntRangeSlider(description=f'Y Crop')\n",
    "\n",
    "checkbox_accordA = widgets.Checkbox(value=False, description='Enabled')\n",
    "\n",
    "\n",
    "\n",
    "### Defining the Filter Accordion ###\n",
    "arcordion_crop_comps = widgets.VBox([checkbox_accordA,\n",
    "                                        title_ABE, \n",
    "                                        range_slider_accordA_x_ABE, range_slider_accordA_y_ABE,\n",
    "                                        title_253, \n",
    "                                        range_slider_accordA_x_253, range_slider_accordA_y_253,\n",
    "                                        button_recommended_fparams])\n",
    "\n",
    "accordion_children = [arcordion_crop_comps]\n",
    "\n",
    "\n",
    "\n",
    "accordion = widgets.Accordion(children=accordion_children)\n",
    "accordion.selected_index = None\n",
    "for i, title in enumerate(['Crop']):\n",
    "    accordion.set_title(index=i,title=title)\n",
    "    \n",
    "##########################\n",
    "### Handpick Accordion ###\n",
    "########################## \n",
    "\n",
    "checkbox_handpick = widgets.Checkbox(value=False, description='Enabled')\n",
    "\n",
    "accordion_handpick_input_size = widgets.SelectionSlider(options=[128, 256, 512, 1024], value=512, description='Sample Size:')\n",
    "\n",
    "accordion_handpick_input_Xoffset = widgets.IntSlider(value=1, min=0, max=1, description=f'Xoffset')\n",
    "accordion_handpick_input_Yoffset = widgets.IntSlider(value=1, min=0, max=1, description=f'Yoffset')\n",
    "\n",
    "accordion_handpick_button_downlaod = widgets.Button(description='Downl. Sample')\n",
    "    \n",
    "### Defining the Handpick Accordion ###\n",
    "accordion_handpick_children =  widgets.VBox([checkbox_handpick, accordion_handpick_input_size, \n",
    "                                             accordion_handpick_input_Xoffset, accordion_handpick_input_Yoffset,\n",
    "                                             accordion_handpick_button_downlaod])\n",
    "accordion_handpick = widgets.Accordion(children=[accordion_handpick_children])\n",
    "accordion_handpick.selected_index = None\n",
    "for i, title in enumerate(['Handpick Samples']):\n",
    "    accordion_handpick.set_title(index=i,title=title)\n",
    "\n",
    "    \n",
    "#########################\n",
    "### Utility Functions ### \n",
    "######################### \n",
    "\n",
    "def get_messreihe_str():\n",
    "    if input_messreihe.value < 10:\n",
    "        str_ = '00'\n",
    "    elif input_messreihe.value < 100:\n",
    "        str_ = '0' \n",
    "    else:\n",
    "        str_ = ''\n",
    "    return str_ + str(input_messreihe.value) # --> '020'/'005'/'120'\n",
    "\n",
    "def get_all_component_values():\n",
    "    params = {'cam_id' : dropdown.value,\n",
    "              'base_dir_path' : input_path.value,\n",
    "              'messreihe' : get_messreihe_str(),\n",
    "              'pic_num' : input_pic_num.value\n",
    "             }\n",
    "    return params\n",
    "\n",
    "def apply_filters(data):\n",
    "    # data: np.array shape (2056, 2464, 1)\n",
    "    cam_id = dropdown.value\n",
    "    if checkbox_accordA.value:\n",
    "        if cam_id == 'ABE':\n",
    "            min_x, max_x = range_slider_accordA_x_ABE.value\n",
    "            min_y, max_y = range_slider_accordA_y_ABE.value\n",
    "        elif cam_id == '253':\n",
    "            min_x, max_x = range_slider_accordA_x_253.value\n",
    "            min_y, max_y = range_slider_accordA_y_253.value\n",
    "        data = data[min_y:max_y, min_x:max_x, :]\n",
    "    return data\n",
    "\n",
    "def apply_handpick_cropping(data):\n",
    "    img_width = img_height = accordion_handpick_input_size.value # smaple images have to be squares!\n",
    "    Xoffset = accordion_handpick_input_Xoffset.value\n",
    "    Yoffset = accordion_handpick_input_Yoffset.value\n",
    "    data = data[Yoffset:Yoffset+img_height, Xoffset:Xoffset+img_width, :]\n",
    "    return data\n",
    "\n",
    "def get_plot_data():\n",
    "    params = get_all_component_values()\n",
    "    data = load_pic_data(params['messreihe'], \n",
    "                         params['base_dir_path'], \n",
    "                         params['cam_id'], \n",
    "                         params['pic_num'])\n",
    "    return data\n",
    "\n",
    "def get_handpick_data():\n",
    "    params = get_all_component_values()\n",
    "    data = load_pic_data(params['messreihe'], \n",
    "                         params['base_dir_path'], \n",
    "                         params['cam_id'], \n",
    "                         params['pic_num'])\n",
    "    return data\n",
    "\n",
    "def display_img(data):\n",
    "    display_size_perc = input_disp_size.value\n",
    "    height = int(data.shape[0]*display_size_perc)\n",
    "    width  = int(data.shape[1]*display_size_perc)\n",
    "    \n",
    "    data = apply_filters(data)\n",
    "    \n",
    "    pil_image = PIL_Image.fromarray(data.squeeze(), mode='L')\n",
    "    image_buffer = io.BytesIO()\n",
    "    pil_image.save(image_buffer, format='PNG')\n",
    "    ipy_img = IPython_Image(data=image_buffer.getvalue(), height=height, width=width) \n",
    "    display(ipy_img)\n",
    "\n",
    "def display_handpick_image(data):\n",
    "    display_size_perc = input_disp_size.value\n",
    "    height = int(data.shape[0]*display_size_perc)\n",
    "    width  = int(data.shape[1]*display_size_perc)\n",
    "    \n",
    "    data = apply_filters(data)\n",
    "    data = apply_handpick_cropping(data)\n",
    "    \n",
    "    pil_image = PIL_Image.fromarray(data.squeeze(), mode='L')\n",
    "    image_buffer = io.BytesIO()\n",
    "    pil_image.save(image_buffer, format='PNG')\n",
    "    ipy_img = IPython_Image(data=image_buffer.getvalue(), height=height, width=width) \n",
    "    display(ipy_img)\n",
    "    \n",
    "    \n",
    "    \n",
    "###########################\n",
    "### Callbacks Functions ### \n",
    "########################### \n",
    "\n",
    "def update_min_max_pic_num(temp):\n",
    "    # Get initial data\n",
    "    params = get_all_component_values()\n",
    "    \n",
    "    dir_name  = [name for name in os.listdir(params['base_dir_path']) \n",
    "                      if f\"_{params['messreihe']}_\" in name][-1] # -1 for latest experiment retake\n",
    "    dir_path  = f\"{params['base_dir_path']}/{dir_name}\"\n",
    "    file_names = [name for name in os.listdir(dir_path) \n",
    "                    if f\"{params['cam_id']})\" in name]\n",
    "    # file_names like ['Camera(...253)_8023_12346161000.npy # --> 8023 is the pic num\n",
    "    \n",
    "    pic_nums = [int(str_.split('_')[2]) for str_ in file_names]\n",
    "    input_pic_num.min = min(pic_nums)\n",
    "    input_pic_num.max = max(pic_nums)\n",
    "\n",
    "def update_pic_plot(trigger_cls):\n",
    "    data = get_plot_data()\n",
    "    if data is np.nan:\n",
    "        pass\n",
    "    else:\n",
    "        with output_pic: \n",
    "            clear_output(wait=True)\n",
    "            display_img(data)\n",
    "            \n",
    "        if checkbox_handpick.value == True:\n",
    "            with handpick_pic:\n",
    "                clear_output(wait=True)\n",
    "                display_handpick_image(data)\n",
    "            \n",
    "\n",
    "    \n",
    "def update_only_hand_pic_plot_with_filters(temp):\n",
    "    # Like update_pic_plot, but only updating the handpick image\n",
    "    params = get_all_component_values()\n",
    "    if checkbox_handpick.value == True:\n",
    "        data = get_plot_data()\n",
    "        if data is np.nan:\n",
    "            with handpick_pic:\n",
    "                print(f\"Warning: no image with num {params['pic_num']} available for handpicking!\")\n",
    "        else:\n",
    "            with handpick_pic:\n",
    "                clear_output(wait=True)\n",
    "                display_handpick_image(data)\n",
    "        \n",
    "\n",
    "def update_pic_plot_with_filters(change):\n",
    "    # wrapper function that updates only if all condiations are met\n",
    "    checkbox_enabled = checkbox_accordA.value\n",
    "    check_box_change = change.owner.description == 'Enabled'\n",
    "    requirement = checkbox_enabled or check_box_change\n",
    "    if requirement: \n",
    "        update_pic_plot(None)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def download_img(trigger_cls):\n",
    "    params = get_all_component_values()\n",
    "    data = get_plot_data()\n",
    "    if data is np.nan:\n",
    "        pass\n",
    "    else:\n",
    "        data = apply_filters(data)\n",
    "        data = np.squeeze(data, axis=-1) # shape (2056, 2464, 1) --> (2056, 2464)\n",
    "        pil_img = PIL_Image.fromarray(data, mode='L')    \n",
    "        pil_img.save(f\"img_{params['pic_num']}_{params['cam_id']}.png\")\n",
    "\n",
    "def download_handpick_img(temp):\n",
    "    params = get_all_component_values()\n",
    "    data = get_plot_data()\n",
    "    if data is np.nan:\n",
    "        with handpick_pic:\n",
    "                print(f\"Warning: no image with num {params['pic_num']} available for handpicking!\")\n",
    "    else:\n",
    "        data = apply_filters(data)\n",
    "        data = apply_handpick_cropping(data)\n",
    "        data = np.squeeze(data, axis=-1) # shape (512, 512, 1) --> (512, 512)\n",
    "        assert data.shape[0] == data.shape[1] # Check if data is really square\n",
    "        pil_img = PIL_Image.fromarray(data, mode='L')    \n",
    "        \n",
    "        dir_ = os.getcwd()  # home dir where images are saved\n",
    "        substr = f\"_{params['messreihe']}_{params['cam_id']}\"\n",
    "        files = os.listdir(dir_)     \n",
    "        matches = [file for file in files if substr in file]\n",
    "        img_num = len(matches) + 1\n",
    "        if params['messreihe'] == '020': # different concentrate pic nums for other mesurement rows!\n",
    "            if  params['pic_num'] < 2920:\n",
    "                concentrate = 1\n",
    "            elif params['pic_num'] < 4720:\n",
    "                concentrate = 2\n",
    "            elif params['pic_num'] < 11860:\n",
    "                concentrate = 3\n",
    "            elif params['pic_num'] < 22000:\n",
    "                concentrate = 4\n",
    "        else:\n",
    "            concentrate = '?' # unicode ?\n",
    "            \n",
    "        pil_img.save(f\"img{img_num}_{params['pic_num']}_{params['messreihe']}_{params['cam_id']}_c{concentrate}.tif\") # ? for number 1-4 (number of the concentrate) \n",
    "            # pictures for stardist have to be .tif files\n",
    "        \n",
    "def use_recommended_params(temp):\n",
    "    range_slider_accordA_x_ABE.value = (820, 2464)\n",
    "    range_slider_accordA_y_ABE.value = (100, 1860)\n",
    "    range_slider_accordA_x_253.value = (780, 2464)\n",
    "    range_slider_accordA_y_253.value = (0, 2056)\n",
    "    checkbox_accordA.value = True # prevent too many updates\n",
    "    \n",
    "    \n",
    "def update_handpick_sliders(temp):\n",
    "    ### Handpick offset sliders\n",
    "    params = get_all_component_values() # considering different cropping sizes for cams ABE and 253 \n",
    "    data = load_pic_data(params['messreihe'], params['base_dir_path'], params['cam_id'], params['pic_num']) \n",
    "    data = apply_filters(data) # get the crop widths and heights \n",
    "    max_height  = data.shape[0] \n",
    "    max_width   = data.shape[1] \n",
    "    min_height  = 0\n",
    "    min_width   = 0\n",
    "\n",
    "    accordion_handpick_input_Xoffset.max = max_width  - accordion_handpick_input_size.value\n",
    "    accordion_handpick_input_Yoffset.max = max_height - accordion_handpick_input_size.value\n",
    "    accordion_handpick_input_Xoffset.min = min_height\n",
    "    accordion_handpick_input_Yoffset.min = min_width\n",
    "\n",
    "    \n",
    "######################\n",
    "### Init Functions ###\n",
    "######################\n",
    "\n",
    "def init_components():\n",
    "    params = get_all_component_values()\n",
    "    data = load_pic_data(params['messreihe'], \n",
    "                         params['base_dir_path'], \n",
    "                         params['cam_id'], \n",
    "                         params['pic_num'])\n",
    "    \n",
    "    dir_name  = [name for name in os.listdir(params['base_dir_path']) \n",
    "                      if f\"_{params['messreihe']}_\" in name][-1] # -1 for latest experiment retake\n",
    "    dir_path  = f\"{params['base_dir_path']}/{dir_name}\"\n",
    "    file_names = [name for name in os.listdir(dir_path) \n",
    "                    if f\"{params['cam_id']})\" in name]\n",
    "    \n",
    "    ### Pic num Slider\n",
    "    update_min_max_pic_num(None)\n",
    "    \n",
    "    ### Crop Filter\n",
    "    sample_data = load_pic_data(params['messreihe'], params['base_dir_path'], \n",
    "                                params['cam_id'], params['pic_num'])\n",
    "    max_height  = sample_data.shape[0] # ABE and 253 have same pic size!\n",
    "    max_width   = sample_data.shape[1] # shape like (2056, 2464, 1)\n",
    "    \n",
    "    range_slider_accordA_x_ABE.max = max_width\n",
    "    range_slider_accordA_y_ABE.max = max_height\n",
    "    range_slider_accordA_x_253.max = max_width\n",
    "    range_slider_accordA_y_253.max = max_height\n",
    "    \n",
    "    range_slider_accordA_x_ABE.value = (0, max_width )\n",
    "    range_slider_accordA_y_ABE.value = (0, max_height)\n",
    "    range_slider_accordA_x_253.value = (0, max_width )\n",
    "    range_slider_accordA_y_253.value = (0, max_height)\n",
    "    \n",
    "    ### Handpick sliders\n",
    "    update_handpick_sliders(None)\n",
    "\n",
    "#######################\n",
    "### Specify Outputs ###\n",
    "#######################\n",
    "\n",
    "title_output = widgets.Output() # for printing titles\n",
    "output_pic   = widgets.Output() # image display canvas\n",
    "handpick_pic = widgets.Output() # seconds image display for handpicking subsamples\n",
    "debug_output = widgets.Output() # for printing\n",
    "\n",
    "#########################\n",
    "### Callback Triggers ###\n",
    "#########################\n",
    "\n",
    "dropdown.observe(update_min_max_pic_num, names='value') # update on change\n",
    "dropdown.observe(update_pic_plot, names='value') # update on change\n",
    "input_path.observe(update_min_max_pic_num, names='value') \n",
    "input_messreihe.observe(update_min_max_pic_num, names='value') \n",
    "input_messreihe.observe(update_pic_plot, names='value') \n",
    "input_disp_size.observe(update_pic_plot, names='value')\n",
    "input_pic_num.observe(update_pic_plot, names='value') \n",
    "accordion_handpick_input_size.observe(update_handpick_sliders, 'value')\n",
    "accordion_handpick_input_size.observe(update_only_hand_pic_plot_with_filters, 'value')\n",
    "\n",
    "for VBox_comp in accordion.children:\n",
    "    for comp in VBox_comp.children:\n",
    "        comp.observe(update_pic_plot_with_filters, names='value') \n",
    "for VBox_comp in accordion_handpick.children:\n",
    "    for comp in VBox_comp.children:\n",
    "        comp.observe(update_only_hand_pic_plot_with_filters, names='value')  \n",
    "    \n",
    "accordion_handpick_button_downlaod.on_click(download_handpick_img)\n",
    "button_save.on_click(download_img) \n",
    "button_recommended_fparams.on_click(use_recommended_params)\n",
    "\n",
    "########################\n",
    "### Preparing Layout ###\n",
    "########################\n",
    "\n",
    "inputs_tab1 = widgets.VBox([dropdown, input_path, input_messreihe, input_pic_num, input_disp_size,\n",
    "                            button_save, accordion, accordion_handpick])\n",
    "outputs_tab1 = widgets.VBox([title_output, output_pic, handpick_pic, debug_output])\n",
    "init_components() # display updated initial params based on standard values\n",
    "update_pic_plot(None) # always display an initial image\n",
    "\n",
    "tab1_content = widgets.HBox([inputs_tab1, outputs_tab1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f90ad1-53bb-4da1-956f-fb148d0f1eeb",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43696a23-7c27-4b58-8c41-4d3981bef6b0",
   "metadata": {},
   "source": [
    "#### Tab 2: Model Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eeb90fd-069f-4037-822b-756af189082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = widgets.Text(value='/home/beyer83/data', description='Data Path:')\n",
    "input_model_name = widgets.Text(value='model_ABE_c2,c3,c4', description='Model Name:')\n",
    "input_disp_size2 = widgets.FloatSlider(value=0.5, min=0.4, max=1, step=0.01, description='Display Size (%):')\n",
    "\n",
    "###########################\n",
    "### Defining Accordions ###\n",
    "###########################\n",
    "\n",
    "### Data Augmentation ###\n",
    "button_sample_augmentation = widgets.Button(description='Sample Augmentation')\n",
    "\n",
    "### Model Training ###\n",
    "accordion_train_epochs     = widgets.IntSlider(value=400, min=1, max=1000, description=f'Epochs:')\n",
    "accordion_train_batch_size = widgets.SelectionSlider(options=[1,2,4,8,16,3,64,128,256,512,1024], value=16, description='Batch Size:')\n",
    "accordion_model_n_rays     = widgets.SelectionSlider(options=[8,16,32,64,128], value=32, description='n_rays:')\n",
    "checkbox_use_gpu           = widgets.Checkbox(value=False, description='Use GPU')\n",
    "button_model_training      = widgets.Button(description='Train Model')\n",
    "\n",
    "### Model Inference ###\n",
    "input_model_name = widgets.Text(value='model_ABE_c2,c3,c4', description='Model Name:')\n",
    "title_select_picture = widgets.HTML(\"<h2>Select a Picture:</h2>\")\n",
    "dropdown2        = widgets.Dropdown(options=['ABE', '253'], description='Camera ID:')\n",
    "input_path2      = widgets.Text(value='/home/beyer83/flotsim/Camera_Outotec', description='base_dir_path:')\n",
    "input_messreihe2 = widgets.IntSlider(value=20, min=1, max=62, description='Experiment Num.:')\n",
    "input_pic_num2   = widgets.IntSlider(value=1, description='Picture num:')\n",
    "input_disp_size2 = widgets.FloatSlider(value=0.25, min=0.01, max=1, step=0.01, description='Display Size (%):')\n",
    "button_model_predict = widgets.Button(description='Load & Predict')\n",
    "checkbox_auto_crop  = widgets.Checkbox(value=True, description='Auto Crop')\n",
    "\n",
    "arcordion_data_aug = widgets.VBox([button_sample_augmentation])\n",
    "arcordion_train    = widgets.VBox([accordion_train_epochs, accordion_train_batch_size, accordion_model_n_rays, \n",
    "                                   checkbox_use_gpu, button_model_training])\n",
    "arcordion_predict  = widgets.VBox([title_select_picture, dropdown2, input_path2, input_messreihe2,\n",
    "                                   input_pic_num2, input_disp_size2, checkbox_auto_crop, button_model_predict]) \n",
    "accordion_tab2_children = [arcordion_data_aug, arcordion_train, arcordion_predict]\n",
    "                     \n",
    "accordion_tab2 = widgets.Accordion(children=[arcordion_data_aug, arcordion_train, arcordion_predict])\n",
    "accordion_tab2.selected_index = None\n",
    "for i, title in enumerate(['Data Preview', 'Model Training', 'Model Inference']):\n",
    "    accordion_tab2.set_title(index=i,title=title)\n",
    "\n",
    "#########################\n",
    "### Utility Functions ###\n",
    "#########################\n",
    "\n",
    "def get_messreihe_str_tab2():\n",
    "    if input_messreihe2.value < 10:\n",
    "        str_ = '00'\n",
    "    elif input_messreihe2.value < 100:\n",
    "        str_ = '0' \n",
    "    else:\n",
    "        str_ = ''\n",
    "    return str_ + str(input_messreihe2.value) # --> '020'/'005'/'120'\n",
    "\n",
    "\n",
    "def get_all_component_values_tab2():\n",
    "    params = {'cam_id' : dropdown2.value,\n",
    "              'base_dir_path' : input_path2.value,\n",
    "              'messreihe' : get_messreihe_str_tab2(),\n",
    "              'pic_num' : input_pic_num2.value\n",
    "             }\n",
    "    return params\n",
    "\n",
    "def image_preprocessing_stardist(x_img_arr, y_img_arr):\n",
    "\n",
    "    axis_norm = (0,1)   # normalize channels independently\n",
    "    # axis_norm = (0,1,2) # normalize channels jointly\n",
    "    x_img_arr = [normalize(x,1,99.8,axis=axis_norm) for x in x_img_arr]\n",
    "    y_img_arr = [fill_label_holes(y) for y in y_img_arr]\n",
    "    return x_img_arr, y_img_arr\n",
    "    \n",
    "def augmenter(x_img, y_img):\n",
    "    \"\"\"Input: single images\"\"\"\n",
    "    transform = A.Compose([\n",
    "        A.OneOf([\n",
    "            A.OneOf([\n",
    "                    A.Flip(),\n",
    "                    A.Transpose(),\n",
    "                ], p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=(0,0), rotate_limit=360),\n",
    "            A.OneOf([\n",
    "                    A.Sharpen(alpha=(0.2, 0.75)),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.20, contrast_limit=0.75),            \n",
    "                ], p=0.5),\n",
    "            A.OneOf([\n",
    "                A.MultiplicativeNoise(multiplier=(0.75, 1.25)),\n",
    "            ], p=0.5),\n",
    "            A.OneOf([\n",
    "                    A.MotionBlur(),\n",
    "                    A.MedianBlur(blur_limit=3),\n",
    "                    A.Blur(blur_limit=3),\n",
    "                ], p=0.4),\n",
    "            A.OneOf([\n",
    "                    A.OpticalDistortion(),\n",
    "                    A.GridDistortion(),\n",
    "                    A.ElasticTransform(),\n",
    "                ], p=1)\n",
    "            ], p=0.95), # 5 % of images stay unaffected\n",
    "        ])\n",
    "    trans = transform(image=x_img,mask=y_img)\n",
    "    img_x, img_y = trans['image'], trans['mask']\n",
    "    return img_x, img_y\n",
    "        \n",
    "def get_config():\n",
    "    n_rays = accordion_model_n_rays.value \n",
    "    grid = (16,16) # These are the sizes of all the convolution grids that get encoded? # Etremely high memory demand for (256,256)\n",
    "    use_gpu = checkbox_use_gpu.value and gputools_available()\n",
    "    n_channel_in = 1 \n",
    "    train_patch_size = (512,512) # Predict on ONE subsampled 512x512 grid for increased efficiency and larger field of view; 256x256 for 4 grids (but too small for some bubbles)\n",
    "    train_epochs = accordion_train_epochs.value\n",
    "    train_batch_size = accordion_train_batch_size.value \n",
    "\n",
    "    config = Config2D (\n",
    "        n_rays       = n_rays,\n",
    "        grid         = grid,\n",
    "        use_gpu      = use_gpu,\n",
    "        n_channel_in = n_channel_in,\n",
    "        train_patch_size = train_patch_size,\n",
    "        train_epochs = train_epochs\n",
    "    )\n",
    "    return config\n",
    "\n",
    "def model_sanity_check(model, sample_img):\n",
    "    pass\n",
    "    #median_size = calculate_extents(sample_img, np.median)\n",
    "    #fov = np.array(model._axes_tile_overlap('YX'))\n",
    "    #if any(median_size > fov):\n",
    "    #    print(f\"median object size:      {median_size}\")\n",
    "    #    print(f\"network field of view :  {fov}\")\n",
    "    #    print(\"WARNING: median object size larger than field of view of the neural network. Adjust the parameters grid and train_patch_size!\")\n",
    "\n",
    "def get_model_path():\n",
    "    data_path = input_data_path.value\n",
    "    model_name = input_model_name.value \n",
    "    model_path = data_path + '/' + model_name + '/models' # --> '/home/beyer83/data/model_ABE_c2,c3,c4/models'\n",
    "    return model_path\n",
    "\n",
    "def get_model_name():\n",
    "    model_name = input_model_name.value \n",
    "    n_rays = accordion_model_n_rays.value\n",
    "    train_epochs = accordion_train_epochs.value\n",
    "    model_name = model_name + '_' + str(n_rays) + 'r_' + str(train_epochs) + 'epochs'\n",
    "    return model_name\n",
    "        \n",
    "def get_model(config):\n",
    "    data_path = input_data_path.value\n",
    "    model_name = input_model_name.value \n",
    "    n_rays = accordion_model_n_rays.value\n",
    "    train_epochs = accordion_train_epochs.value\n",
    "    \n",
    "    train_x, train_y, test_x, test_y, val_x, val_y = get_train_val_test_data()\n",
    "    model_path = get_model_path()\n",
    "    save_model_name = get_model_name() # model gets saved to {model_path}/{save_model_name}\n",
    "    \n",
    "    conf = get_config() \n",
    "    model = StarDist2D(conf, name=save_model_name, basedir=model_path)\n",
    "    model_sanity_check(model, train_x[0])\n",
    "    return model\n",
    "\n",
    "          \n",
    "def load_model():\n",
    "    model_name = 'model_ABE_c2,c3,c4_32r_400epochs'\n",
    "    base_dir = get_model_path()\n",
    "    print(f'Using hardcoded model {model_name} in {base_dir} for prediction')\n",
    "    model = StarDist2D(config=None, name=model_name, basedir=base_dir)\n",
    "    return model\n",
    "\n",
    "def get_val_metrics(val_y, val_y_pred):\n",
    "    # Validation is done within every Stardist epoch. This is a custom validation method.\n",
    "    print(\"WARNING: No real validation and test data is present at the moment!\")\n",
    "    taus = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    stats = [matching_dataset(val_y, val_y_pred, thresh=t, show_progress=False) for t in tqdm(taus)]\n",
    "    # Plotting implementation see https://github.com/stardist/stardist/blob/master/examples/2D/2_training.ipynb at the bottom\n",
    "    return stats\n",
    "\n",
    "def test(test_x, test_y, model):\n",
    "    print(\"WARNING: No real validation and test data is present at the moment!\")\n",
    "    taus = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    stats = [matching_dataset(test_x, test_y, thresh=t, show_progress=False) for t in tqdm(taus)]\n",
    "    # Plotting implementation see https://github.com/stardist/stardist/blob/master/examples/2D/2_training.ipynb at the bottom\n",
    "    return stats\n",
    "\n",
    "def get_train_val_test_file_paths():\n",
    "    #paths = get_train_val_test_paths()\n",
    "    data_path = input_data_path.value\n",
    "    model_name = input_model_name.value # \n",
    "    model_path = data_path + '/' + model_name # --> '/home/beyer83/data/model_ABE_c2,c3,c4'\n",
    "    sub_dirs = ['/train/images', '/train/masks', '/test/images', '/test/masks', '/val/images', '/val/masks']\n",
    "    paths = [model_path + dir_ for dir_ in (sub_dirs)]\n",
    "    assert all(os.path.exists(path) for path in paths)\n",
    "    files = [os.listdir(path_) for path_ in paths] \n",
    "    file_paths = [[path + '/' + file for file in files if '.tif' in file] for path, files in zip(paths, files)]\n",
    "    return file_paths # returns paths for train_x, train_y, test_x, test_y, val_x, val_y   \n",
    "\n",
    "def get_train_val_test_data():\n",
    "    file_paths = get_train_val_test_file_paths()\n",
    "    data  = [[imread(file_path) for file_path in file_paths_list] for file_paths_list in file_paths]\n",
    "    return data # unpack returned data with: train_x, train_y, test_x, test_y, val_x, val_y = get_train_val_test_data()\n",
    "\n",
    "def get_selected_image(): # returns one of the image from the entire dataset based on the widget selections\n",
    "    params_tab2 = get_all_component_values_tab2()\n",
    "    data = load_pic_data(params_tab2['messreihe'], \n",
    "                         params_tab2['base_dir_path'], \n",
    "                         params_tab2['cam_id'], \n",
    "                         params_tab2['pic_num'])\n",
    "    return data\n",
    "\n",
    "# model.predict_instances needs a normalizer class\n",
    "class MyNormalizer(Normalizer):\n",
    "    def __init__(self, mi, ma):\n",
    "            self.mi, self.ma = mi, ma\n",
    "    def before(self, x, axes):\n",
    "        return normalize_mi_ma(x, self.mi, self.ma, dtype=np.float32)\n",
    "    def after(*args, **kwargs):\n",
    "        assert False\n",
    "    @property\n",
    "    def do_after(self):\n",
    "        return False    \n",
    "    \n",
    "def display_img_tab2(data):\n",
    "    display_size_perc = input_disp_size2.value\n",
    "    height = int(data.shape[0]*display_size_perc)\n",
    "    width  = int(data.shape[1]*display_size_perc)\n",
    "    \n",
    "    pil_image = PIL_Image.fromarray(data.squeeze(), mode='L')\n",
    "    image_buffer = io.BytesIO()\n",
    "    pil_image.save(image_buffer, format='PNG')\n",
    "    ipy_img = IPython_Image(data=image_buffer.getvalue(), height=height, width=width) \n",
    "    display(ipy_img)\n",
    "        \n",
    "##########################\n",
    "### Callback Functions ###\n",
    "##########################\n",
    "    \n",
    "def display_raw_vs_augmented(temp): \n",
    "    display_size_perc = input_disp_size2.value\n",
    "    lbl_cmap = random_label_cmap()\n",
    "    pic_num = 0\n",
    "    train_x, train_y, test_x, test_y, val_x, val_y = get_train_val_test_data()\n",
    "    x, y = train_x, train_y \n",
    "    if len(x) == 0: # no rendering on no data\n",
    "        return\n",
    "    \n",
    "    x, y  = image_preprocessing_stardist(x, y)\n",
    "    x, y  = x[pic_num], y[pic_num]\n",
    "    x_aug, y_aug  = augmenter(x, y)\n",
    "    fig = plt.figure(figsize=(int(32*display_size_perc), int(20*display_size_perc)))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(int(32*display_size_perc), int(20*display_size_perc)))\n",
    "    ax1.imshow(x, cmap='gray', clim=(0,1)); ax1.axis('off'); ax1.title.set_text('Raw Image')\n",
    "    ax1.imshow(y, cmap=lbl_cmap, alpha=0.2)\n",
    "    \n",
    "    ax2.imshow(x_aug, cmap='gray'); ax2.axis('off'); ax2.title.set_text('Augmented Image')\n",
    "    ax2.imshow(y_aug, cmap=lbl_cmap, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    with output_tab2:\n",
    "        clear_output(wait=True)\n",
    "        plt.show(fig);\n",
    "        \n",
    "def train(temp):\n",
    "    train_x, train_y, test_x, test_y, val_x, val_y = get_train_val_test_data()\n",
    "    del test_x, test_y\n",
    "    config = get_config()\n",
    "    model = get_model(config)\n",
    "    train_x, train_y = image_preprocessing_stardist(train_x, train_y)\n",
    "    val_x,     val_y = image_preprocessing_stardist(val_x, val_y)\n",
    "        \n",
    "    history = model.train(train_x, train_y, validation_data=(val_x, val_y), augmenter=augmenter)\n",
    "    print(\"WARNING: No real validation and test data is present at the moment!\")\n",
    "        #model.optimize_thresholds(val_x, val_y) # If val data is present: Uncomment and use this to improve performance further\n",
    "    print(f\"Model saved to {get_model_path() + '/' + get_model_name()}\")\n",
    "        \n",
    "\n",
    "\n",
    "def display_predictions(temp):\n",
    "    model = load_model()\n",
    "    data_x = get_selected_image()\n",
    "    display_size_perc = input_disp_size2.value*1.5\n",
    "    lbl_cmap = random_label_cmap()\n",
    "    cam_id = dropdown2.value\n",
    "    \n",
    "    try:\n",
    "        if len(data_x) == 0: # no rendering on no data\n",
    "            print('No image available for selection!')\n",
    "            return\n",
    "    except: # sometimes data_x is a float when no picture available\n",
    "        return\n",
    "    \n",
    "    if checkbox_auto_crop.value:\n",
    "        if cam_id == 'ABE':\n",
    "            min_x, max_x = (820, 2464)\n",
    "            min_y, max_y = (100, 1860)\n",
    "        elif cam_id == '253':\n",
    "            min_x, max_x = (780, 2464)\n",
    "            min_y, max_y = (0, 2056)\n",
    "        data_x = data_x[min_y:max_y, min_x:max_x, :]\n",
    "    #axis_norm = (0,1)\n",
    "    #data_x = normalize(data_x, 1,99.8, axis=axis_norm)\n",
    "    #data_x = np.squeeze(data_x, axis=-1) # shape (2056, 2464, 1) --> (2056, 2464)\n",
    "    #print(data_x.shape)\n",
    "    #data_y, polys = model.predict_instances_big(data_x, axes='YX', block_size=512, min_overlap=64, context=64)\n",
    "    \n",
    "    mi, ma = 0, 255                                                   # use min and max dtype values (suitable here)\n",
    "    normalizer_cls = MyNormalizer(mi, ma)\n",
    "    data_y, polys = model.predict_instances(data_x, normalizer=normalizer_cls, n_tiles=(4,4,1))\n",
    "    fig = plt.figure(figsize=(int(32*display_size_perc), int(20*display_size_perc)))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(int(32*display_size_perc), int(20*display_size_perc)))\n",
    "    ax1.imshow(data_x, cmap='gray'); ax1.axis('off'); ax1.title.set_text('Input Image')\n",
    "    \n",
    "    ax2.imshow(data_x, cmap='gray'); ax2.axis('off'); ax2.title.set_text('Predictions')\n",
    "    ax2.imshow(data_y, cmap=lbl_cmap, alpha=0.25)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    with output_tab2:\n",
    "        clear_output(wait=True)\n",
    "        plt.show(fig);\n",
    "\n",
    "def update_pic_plot_tab2(trigger_cls):\n",
    "    data = get_selected_image()\n",
    "    cam_id = dropdown2.value\n",
    "    if data is np.nan:\n",
    "        pass\n",
    "    else:\n",
    "        if checkbox_auto_crop.value:\n",
    "            if cam_id == 'ABE':\n",
    "                min_x, max_x = (820, 2464)\n",
    "                min_y, max_y = (100, 1860)\n",
    "            elif cam_id == '253':\n",
    "                min_x, max_x = (780, 2464)\n",
    "                min_y, max_y = (0, 2056)\n",
    "            data = data[min_y:max_y, min_x:max_x, :]\n",
    "        with output_tab2: \n",
    "            clear_output(wait=True)\n",
    "            display_img_tab2(data)\n",
    "        \n",
    "def update_min_max_pic_num_tab2(temp):\n",
    "    # Get initial data\n",
    "    params_tab2 = get_all_component_values_tab2()\n",
    "    \n",
    "    dir_name  = [name for name in os.listdir(params_tab2['base_dir_path']) \n",
    "                      if f\"_{params_tab2['messreihe']}_\" in name][-1] # -1 for latest experiment retake\n",
    "    dir_path  = f\"{params_tab2['base_dir_path']}/{dir_name}\"\n",
    "    file_names = [name for name in os.listdir(dir_path) \n",
    "                    if f\"{params_tab2['cam_id']})\" in name]\n",
    "    # file_names like ['Camera(...253)_8023_12346161000.npy # --> 8023 is the pic num\n",
    "    \n",
    "    pic_nums = [int(str_.split('_')[2]) for str_ in file_names]\n",
    "    input_pic_num2.min = min(pic_nums)\n",
    "    input_pic_num2.max = max(pic_nums)\n",
    "\n",
    "        \n",
    "#######################\n",
    "### Specify Outputs ###\n",
    "#######################\n",
    "\n",
    "######################\n",
    "### Init Functions ###\n",
    "######################\n",
    "\n",
    "def init_tab2_components():\n",
    "    update_min_max_pic_num_tab2(None)\n",
    "\n",
    "output_tab2   = widgets.Output()\n",
    "\n",
    "#########################\n",
    "### Callback Triggers ###\n",
    "#########################\n",
    "\n",
    "button_sample_augmentation.on_click(display_raw_vs_augmented) \n",
    "button_model_training.on_click(train) \n",
    "button_model_predict.on_click(display_predictions) \n",
    "\n",
    "dropdown2.observe(update_min_max_pic_num_tab2, names='value') # update on change\n",
    "dropdown2.observe(update_pic_plot_tab2, names='value') # update on change\n",
    "input_path2.observe(update_min_max_pic_num_tab2, names='value') \n",
    "input_messreihe2.observe(update_min_max_pic_num_tab2, names='value') \n",
    "input_messreihe2.observe(update_pic_plot_tab2, names='value') \n",
    "input_disp_size2.observe(update_pic_plot_tab2, names='value')\n",
    "input_pic_num2.observe(update_pic_plot_tab2, names='value') \n",
    "\n",
    "########################\n",
    "### Preparing Layout ###\n",
    "########################\n",
    "\n",
    "inputs_tab2 = widgets.VBox([input_data_path, input_model_name, input_disp_size2, accordion_tab2])\n",
    "outputs_tab2 = widgets.VBox([output_tab2])\n",
    "tab2_content = widgets.HBox([inputs_tab2, outputs_tab2])\n",
    "init_tab2_components()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164577e-1d27-496f-9228-780af75f6e68",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86036330-27d9-41f4-ad3e-453319966021",
   "metadata": {},
   "source": [
    "#### Run Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55efa1-16bf-4e07-b518-73527fb306df",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e0fee1-75ef-47b9-9398-dccdc99f9958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9bcb5424c74ad6a60bb8454cd3e177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(VBox(children=(Dropdown(description='Camera ID:', options=('ABE', '253'), value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################\n",
    "### Run Dashboard ###\n",
    "#####################\n",
    "tab = widgets.Tab([tab1_content, tab2_content])\n",
    "titles = ['EDA + Filters', 'Model Inference']\n",
    "for i, title in enumerate(titles):\n",
    "    tab.set_title(i, title)\n",
    "\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2204e6-8a4f-400a-a8fd-f371b48b0e53",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87c19e-a0d2-4d97-b0f8-e9cec0484cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42d29b-1c76-496d-b84c-5701fd16a381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16558595-8f8c-4cc1-8ecd-7a6cbb6418bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubbleseg_fix",
   "language": "python",
   "name": "bubbleseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
